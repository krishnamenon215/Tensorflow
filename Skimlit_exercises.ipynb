{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.Train model_5 on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
        "\n",
        "*   tf.keras.callbacks.ModelCheckpoint to save the model's best weights only.\n",
        "*   tf.keras.callbacks.EarlyStopping to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h0yK_1ljZd6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get the data"
      ],
      "metadata": {
        "id": "6MP02nhsZ9jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG1Xr6aSaKK4",
        "outputId": "abb8d74a-afb1-4cdd-81cc-076f01e95806"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Start experiment using the 20k dataset with numbers replcaed by \"@\" sign\n",
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "metadata": {
        "id": "OqsZQX9raMof"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check all the filenames in the target directory\n",
        "import os\n",
        "filenames =[data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frzhhPD3acSN",
        "outputId": "9d6fd1cf-b2f4-4080-a43d-3c987efba0a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocess the data"
      ],
      "metadata": {
        "id": "br2QRL9maerE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  with open(filename,'r') as f:\n",
        "    return f.readlines()"
      ],
      "metadata": {
        "id": "1NiKRtgAaodP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets read in training lines\n",
        "train_lines = get_lines(data_dir + \"train.txt\")\n",
        "train_lines[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8ACtWQzao_r",
        "outputId": "555989b2-8185-4626-cdf1-9d06bc8e4fbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['###24293578\\n',\n",
              " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
              " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
              " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
              " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
              " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
              " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
              " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
              " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
              " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
              " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
              " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
              " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
              " '\\n',\n",
              " '###24854809\\n',\n",
              " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
              " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
              " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
              " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
              " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to represet the lines as dictionary"
      ],
      "metadata": {
        "id": "0LGZBOcmase4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "  input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "  \n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    \n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "  \n",
        "  return abstract_samples"
      ],
      "metadata": {
        "id": "supiyyREa31a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
        "print(len(train_samples),len(val_samples),len(test_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuKQm_-7a44B",
        "outputId": "77c3d831-aef3-4798-8a19-090b58fb2584"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180040 30212 30135\n",
            "CPU times: user 679 ms, sys: 137 ms, total: 816 ms\n",
            "Wall time: 1.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting the dictionary to dataframe"
      ],
      "metadata": {
        "id": "1Ysju3FMa73l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8GVSVp6GbCb9",
        "outputId": "bd9934b6-af5c-439f-8112-e3c76a18c962"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         target                                               text  \\\n",
              "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
              "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
              "2       METHODS  outcome measures included pain reduction and i...   \n",
              "3       METHODS  pain was assessed using the visual analog pain...   \n",
              "4       METHODS  secondary outcome measures included the wester...   \n",
              "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
              "6       RESULTS  there was a clinically relevant reduction in t...   \n",
              "7       RESULTS  the mean difference between treatment arms ( @...   \n",
              "8       RESULTS  further , there was a clinically relevant redu...   \n",
              "9       RESULTS  these differences remained significant at @ we...   \n",
              "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
              "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
              "12   BACKGROUND  emotional eating is associated with overeating...   \n",
              "13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n",
              "\n",
              "    line_number  total_lines  \n",
              "0             0           11  \n",
              "1             1           11  \n",
              "2             2           11  \n",
              "3             3           11  \n",
              "4             4           11  \n",
              "5             5           11  \n",
              "6             6           11  \n",
              "7             7           11  \n",
              "8             8           11  \n",
              "9             9           11  \n",
              "10           10           11  \n",
              "11           11           11  \n",
              "12            0           10  \n",
              "13            1           10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7df6cfed-3ea6-4641-9afd-1e6a9ac87ea7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the outcome measures in rheumatology clinical ...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>emotional eating is associated with overeating...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>yet , empirical evidence for individual ( trai...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7df6cfed-3ea6-4641-9afd-1e6a9ac87ea7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7df6cfed-3ea6-4641-9afd-1e6a9ac87ea7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7df6cfed-3ea6-4641-9afd-1e6a9ac87ea7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get list of sentences"
      ],
      "metadata": {
        "id": "kaPFzUCHbEji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#COnvert abstract text lines into lists\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOC6wC1dbLkj",
        "outputId": "862fb176-b570-40d7-d23a-6ff08cc2a455"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label encoding encoding for target"
      ],
      "metadata": {
        "id": "OA0y-kJVbL-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract labels and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "#check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfDeZzlPbP1I",
        "outputId": "2a2eed27-e2ed-4781-d7a9-6843fb9308c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get class names and number of classes from LabelEncoder Instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxw282dLbVlT",
        "outputId": "c43104fa-2e71-4466-b399-b153ce808f42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "s7YXyXk4bjM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create positional embeddings"
      ],
      "metadata": {
        "id": "sn8J-PjIbspL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4fhesTJob7mu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COnverting the line_number using onehot encoder"
      ],
      "metadata": {
        "id": "TD6vjJZpenxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "np.percentile(train_df.line_number, 98)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "NpAPSyQ8cPSF",
        "outputId": "c40ceee7-0ae7-42a8-98ff-483f4ee6b350"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bb9870ec37a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the coverage of a \"total_lines\" value of 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors for \"line_number\" column\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "train_line_numbers_one_hot[:14], train_line_numbers_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpyYkoESb4Ck",
        "outputId": "21787c13-e451-4fa8-af30-8c8ac1153069"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(14, 15), dtype=float32, numpy=\n",
              " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "       dtype=float32)>, TensorShape([180040, 15]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting total_lines using onehot encoder"
      ],
      "metadata": {
        "id": "DY3ioFRZetZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "np.percentile(train_df.total_lines, 98)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHksbrmPb6Vp",
        "outputId": "466e8afd-5101-4e6f-cc56-a428d1a590be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors for \"total_lines\" column\n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "train_total_lines_one_hot[:14], train_total_lines_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEHyGiWIcMGd",
        "outputId": "f6588f8a-f0e9-4f8e-fe8a-95137f2112d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(14, 20), dtype=float32, numpy=\n",
              " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.]], dtype=float32)>, TensorShape([180040, 20]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE layer download"
      ],
      "metadata": {
        "id": "26MVVF-Heyg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainable=False,\n",
        "                                        name=\"universal_sentence_encoder\")"
      ],
      "metadata": {
        "id": "RrreAypTdHbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all keyboard characters for char-level embedding\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fef3sD9MdVbq",
        "outputId": "520ff788-25cd-45c6-97cb-8272ba2912fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Split sentences and converting to training,validation and test characters for charater embedding"
      ],
      "metadata": {
        "id": "UVXicaq5e3N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n"
      ],
      "metadata": {
        "id": "INF2-xeaeAbI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "train_chars[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9248NheC0e",
        "outputId": "9357e3ea-3d9d-46e6-9052-7c36760bdd91"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .',\n",
              " 'a   t o t a l   o f   @   p a t i e n t s   w i t h   p r i m a r y   k n e e   o a   w e r e   r a n d o m i z e d   @ : @   ;   @   r e c e i v e d   @   m g / d a y   o f   p r e d n i s o l o n e   a n d   @   r e c e i v e d   p l a c e b o   f o r   @   w e e k s   .',\n",
              " 'o u t c o m e   m e a s u r e s   i n c l u d e d   p a i n   r e d u c t i o n   a n d   i m p r o v e m e n t   i n   f u n c t i o n   s c o r e s   a n d   s y s t e m i c   i n f l a m m a t i o n   m a r k e r s   .',\n",
              " 'p a i n   w a s   a s s e s s e d   u s i n g   t h e   v i s u a l   a n a l o g   p a i n   s c a l e   (   @ - @   m m   )   .',\n",
              " 's e c o n d a r y   o u t c o m e   m e a s u r e s   i n c l u d e d   t h e   w e s t e r n   o n t a r i o   a n d   m c m a s t e r   u n i v e r s i t i e s   o s t e o a r t h r i t i s   i n d e x   s c o r e s   ,   p a t i e n t   g l o b a l   a s s e s s m e n t   (   p g a   )   o f   t h e   s e v e r i t y   o f   k n e e   o a   ,   a n d   @ - m i n   w a l k   d i s t a n c e   (   @ m w d   )   .']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# What's the average character length?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWTR7bo5drai",
        "outputId": "76707bf5-e679-4648-84bc-95b64daaced5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149.3662574983337"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character length covers 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfR6NH_adqHJ",
        "outputId": "7c5a2c68-7e01-41d9-a205-ebf27bd07ce6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorisation"
      ],
      "metadata": {
        "id": "nnHz60Q4fG5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
        "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    # standardize=None, # set standardization to \"None\" if you want to leave punctuation in\n",
        "                                    name=\"char_vectorizer\")\n",
        "# Adapt character vectorizer to training character\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "SYGeZnMTdWRU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding"
      ],
      "metadata": {
        "id": "hkBag4YufJmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocab stats\n",
        "char_vocab = char_vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "15XGxxrkekC4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "# Create char embedding layer\n",
        "char_embed = layers.Embedding(input_dim=len(char_vocab), # number of different characters\n",
        "                              output_dim=25, # this is the size of the char embedding in the paper: https://arxiv.org/pdf/1612.05251.pdf (Figure 1)\n",
        "                              mask_zero=True,\n",
        "                              name=\"char_embed\")"
      ],
      "metadata": {
        "id": "QXLaU_jgef0i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape=(1, ), dtype=\"string\", name=\"char_inputs\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Line numbers model\n",
        "# inputs, shape = ?\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_input\")\n",
        "# dense layer with 32 units & relu activation\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "# combine inputs & dense layer into model\n",
        "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
        "                                   outputs=x)\n",
        "\n",
        "# 4. Total lines model\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_lines_input\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_lines_model = tf.keras.Model(inputs=total_lines_inputs,\n",
        "                                   outputs=y)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"char_token_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embedding with combined token and char embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_model.output,\n",
        "                                                                                 total_lines_model.output,\n",
        "                                                                                 z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# 8. Put together model with all kinds of inputs\n",
        "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                 total_lines_model.input,\n",
        "                                 token_model.input,\n",
        "                                 char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_5_tribrid_embedding_model\")"
      ],
      "metadata": {
        "id": "WKgWZhAkcmE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "5955de6d-32aa-41ab-933d-0b0deec3517a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-886f4f3409c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1. Token inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"token_inputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_hub_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtoken_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m token_model = tf.keras.Model(inputs=token_inputs,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_hub_embedding_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token, char, and positional embedding model\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # helps to prevent overfitting\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "PYPB5MVeiLWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets (with all four kinds of input data)\n",
        "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
        "                                                                train_total_lines_one_hot,\n",
        "                                                                train_sentences,\n",
        "                                                                train_chars))\n",
        "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
        "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Do the same as above but for the validation dataset\n",
        "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
        "                                                              val_total_lines_one_hot,\n",
        "                                                              val_sentences,\n",
        "                                                              val_chars))\n",
        "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
        "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "kcMR6fx5iXhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tf.keras.callbacks.ModelCheckpoint to save the model's best weights only."
      ],
      "metadata": {
        "id": "44Dd1gOkivmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a function to implement a ModelCheckpoint callback with a specific filename \n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            monitor=\"val_loss\",\n",
        "                                            verbose=0, # only output a limited amount of text\n",
        "                                            save_best_only=True)"
      ],
      "metadata": {
        "id": "HVUJckYbc4L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the token, char and positional embedding model\n",
        "history_model_5 = model_5.fit(train_char_token_pos_dataset,\n",
        "                              steps_per_epoch=len(train_char_token_pos_dataset),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_token_pos_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_token_pos_dataset)),\n",
        "                              callbacks=[create_model_checkpoint(model_name=model_5.name)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ5xcPA8icXT",
        "outputId": "88a3cc34-7988-4df6-ee62-49e9f97c49c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5627/5627 [==============================] - ETA: 0s - loss: 0.9538 - accuracy: 0.8223"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5627/5627 [==============================] - 994s 170ms/step - loss: 0.9538 - accuracy: 0.8223 - val_loss: 0.9130 - val_accuracy: 0.8428\n",
            "Epoch 2/3\n",
            "5627/5627 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.8531"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5627/5627 [==============================] - 972s 173ms/step - loss: 0.9097 - accuracy: 0.8531 - val_loss: 0.9046 - val_accuracy: 0.8517\n",
            "Epoch 3/3\n",
            "5627/5627 [==============================] - ETA: 0s - loss: 0.8975 - accuracy: 0.8626"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5627/5627 [==============================] - 986s 175ms/step - loss: 0.8975 - accuracy: 0.8626 - val_loss: 0.9017 - val_accuracy: 0.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Checkout the Keras guide on using pretrained GloVe embeddings. Can you get this working with one of our models?"
      ],
      "metadata": {
        "id": "wJR7UPJci1dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "TP9k76Yg2iJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pre-trained word embeddings"
      ],
      "metadata": {
        "id": "u8Ig84gssK-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split into characters\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]"
      ],
      "metadata": {
        "id": "jcefZPHx2i71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization \n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens= 68000 , \n",
        "                                    output_sequence_length = 56)\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "RO5wMTqf2pqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc = text_vectorizer.get_vocabulary()\n",
        "len(voc)\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "OEnmpZlg3tsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##embedding matrix"
      ],
      "metadata": {
        "id": "GxI_ufAb4MW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVu8iYkCxypc",
        "outputId": "82194af3-cf78-4867-cf09-d8f9e7e048c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOUQGxi3ye4",
        "outputId": "bc121883-1276-45a4-c2b3-7226809d2946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 29730 words (35111 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-HFuNG2tU4E",
        "outputId": "6142e3b7-7b1c-4117-f691-a9e1871b2e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-02 20:06:24--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-02 20:06:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-02 20:06:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: glove.6B.zip\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.88MB/s    in 2m 50s  \n",
            "\n",
            "2023-02-02 20:09:15 (4.84 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = '/content/glove.6B.100d.txt'"
      ],
      "metadata": {
        "id": "EqaNOCyrvMdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##adding embedding layer"
      ],
      "metadata": {
        "id": "lTP_xpDFzyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "tD9s8J104mLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Build"
      ],
      "metadata": {
        "id": "suO8yGGb4nTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample \n",
        "input = layers.Input(shape = (None,) , dtype = 'int64')\n",
        "embedded_sequences = embedding_layer(input)\n",
        "\n",
        "x = layers.Conv1D(128 , 5 , activation= 'relu' , padding = 'same')(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5, padding = 'same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.MaxPooling1D(5 , padding ='same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(len(class_names) , activation= 'softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(input , output)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDRzDMB544P2",
        "outputId": "30fabf42-4dd7-42c8-a04e-31e70dec7c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         6484300   \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, None, 128)         64128     \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,729,681\n",
            "Trainable params: 245,381\n",
            "Non-trainable params: 6,484,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the datasets for our both sentences and chars  \n",
        "\n",
        "train_sen_vectors = text_vectorizer(np.array([[sen] for sen in train_sentences])).numpy()\n",
        "val_sen_vectors = text_vectorizer(np.array([[sen] for sen in val_sentences])).numpy()\n",
        "\n",
        "# Training and validation dataset \n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_sen_vectors , train_labels_encoded))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_sen_vectors , val_labels_encoded))\n",
        "\n",
        "\n",
        "# Applying the batch size and prefetching (performance optimization )\n",
        "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "train_ds,  val_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6gSnyO747Gh",
        "outputId": "f25b51ca-48a6-4452-84b3-ce26372838b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset element_spec=(TensorSpec(shape=(None, 56), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 56), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(train_ds, epochs=3, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_5t3y_H6eZ0",
        "outputId": "db858859-9a89-4a61-fdb1-d07718077812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5627/5627 [==============================] - 95s 17ms/step - loss: 0.7034 - acc: 0.7458 - val_loss: 0.5869 - val_acc: 0.7932\n",
            "Epoch 2/3\n",
            "5627/5627 [==============================] - 93s 17ms/step - loss: 0.6566 - acc: 0.7814 - val_loss: 0.6032 - val_acc: 0.8015\n",
            "Epoch 3/3\n",
            "5627/5627 [==============================] - 96s 17ms/step - loss: 0.6676 - acc: 0.7846 - val_loss: 0.6800 - val_acc: 0.7659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8660c1dc10>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the TensorFlow Hub BERT PubMed expert (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?"
      ],
      "metadata": {
        "id": "qX70t0iz8TuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow_text==2.4.1"
      ],
      "metadata": {
        "id": "RmFrk2eA6J7C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "jGzANzLH50ts"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained TensorFlow Hub Bert PubMed\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "preprocessing = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3' ,\n",
        "                                     trainable = False )\n",
        "\n",
        "bert= hub.KerasLayer('https://tfhub.dev/google/experts/bert/pubmed/2' ,\n",
        "                            trainable = False )"
      ],
      "metadata": {
        "id": "ZpKxlxNJbTV1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "JNZ-YSkoxRaw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the model with the bert layers\\\n",
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "bert_inputs = preprocessing(inputs)\n",
        "bert_embedding = bert(bert_inputs) \n",
        "x = layers.Dense(128, activation=\"relu\")(bert_embedding['pooled_output'])\n",
        "\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x) \n",
        "bert_model = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name=\"model_bert\")"
      ],
      "metadata": {
        "id": "u2XF399AclH-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX_Cwwl_w5Ir",
        "outputId": "825a5b31-7425-411e-aa1e-4af59596226f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_bert\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        {'input_word_ids': ( 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      {'sequence_output':  109482241   keras_layer[0][0]                \n",
            "                                                                 keras_layer[0][1]                \n",
            "                                                                 keras_layer[0][2]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          98432       keras_layer_1[0][13]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            645         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,581,318\n",
            "Trainable params: 99,077\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False) #we want non sparse matrix\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n"
      ],
      "metadata": {
        "id": "5FZBPK4c0qzi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cWRDrfe0JJW",
        "outputId": "7e43af2d-8d74-409d-b642-09045c8d81bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefected datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWw-EoJz1DWi",
        "outputId": "ebaa0487-9809-4797-926c-133cb75bf4de"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "-o_a-cpozTqs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model_2 to the data\n",
        "history_bert = bert_model.fit(train_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1 * len(valid_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1c47v67zDTb",
        "outputId": "11c5fad5-fb93-42a8-ad6f-b9cc5b3e2514"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "562/562 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.7672 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What happens if you were to merge our line_number and total_lines features for each sequence? For example, created a X_of_Y feature instead? Does this effect model performance?\n",
        "Another example: line_number=1 and total_lines=11 turns into line_of_X=1_of_11."
      ],
      "metadata": {
        "id": "3zF-dZewN_3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['line_total'] = train_df['line_number'].astype(str) + '_of_' + train_df['total_lines'].astype(str)\n",
        "val_df['line_total'] = val_df['line_number'].astype(str) + '_of_' + val_df['total_lines'].astype(str)\n",
        "\n",
        "train_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ZaPjIMRBznpA",
        "outputId": "19575eca-e2cd-4359-e175-9646623479e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      target                                               text  line_number  \\\n",
              "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
              "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
              "2    METHODS  outcome measures included pain reduction and i...            2   \n",
              "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
              "4    METHODS  secondary outcome measures included the wester...            4   \n",
              "5    METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...            5   \n",
              "6    RESULTS  there was a clinically relevant reduction in t...            6   \n",
              "7    RESULTS  the mean difference between treatment arms ( @...            7   \n",
              "8    RESULTS  further , there was a clinically relevant redu...            8   \n",
              "9    RESULTS  these differences remained significant at @ we...            9   \n",
              "\n",
              "   total_lines line_total  \n",
              "0           11    0_of_11  \n",
              "1           11    1_of_11  \n",
              "2           11    2_of_11  \n",
              "3           11    3_of_11  \n",
              "4           11    4_of_11  \n",
              "5           11    5_of_11  \n",
              "6           11    6_of_11  \n",
              "7           11    7_of_11  \n",
              "8           11    8_of_11  \n",
              "9           11    9_of_11  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a49b30f-1203-4d14-8387-0dd10af49c84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "      <th>line_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>3_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>4_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>5_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>6_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>8_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>9_of_11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a49b30f-1203-4d14-8387-0dd10af49c84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a49b30f-1203-4d14-8387-0dd10af49c84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a49b30f-1203-4d14-8387-0dd10af49c84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one hot encoding on the train and transform the validation dataframe \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Creating an instance \n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "# Fitting on the training dataframe \n",
        "one_hot_encoder.fit(np.expand_dims(train_df['line_total'] , axis = 1))\n",
        "\n",
        "# Transforming both train and val df \n",
        "train_line_total_one_hot = one_hot_encoder.transform(np.expand_dims(train_df['line_total'] , axis =1))\n",
        "val_line_total_one_hot  = one_hot_encoder.transform(np.expand_dims(val_df['line_total'] , axis= 1))\n",
        "     "
      ],
      "metadata": {
        "id": "CPkBatzqiPt4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the sparse object to array \n",
        "train_line_total_one_hot = train_line_total_one_hot.toarray()\n",
        "val_line_total_one_hot = val_line_total_one_hot.toarray()"
      ],
      "metadata": {
        "id": "kNfX6D4CiZDC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the datatype to int \n",
        "train_line_number_total_encoded = tf.cast(train_line_total_one_hot , dtype= tf.int32)\n",
        "val_line_number_total_encoded = tf.cast(val_line_total_one_hot , dtype= tf.int32)"
      ],
      "metadata": {
        "id": "Zkn7M6VxhhOg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_line_total_one_hot[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KbsfWY9cBZx",
        "outputId": "7bc036ae-77f8-491c-ca16-67d804764b7a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars,train_line_total_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences,val_chars, val_line_total_one_hot))\n",
        "\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(train_labels_encoded)\n",
        "val_labels = tf.data.Dataset.from_tensor_slices(val_labels_encoded)"
      ],
      "metadata": {
        "id": "ricTYR0ncW8Y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefected datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "tKtpTsV3cqa_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the performant datasets for our tribid model \n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sentences ,\n",
        "                                                 train_chars , \n",
        "                                                 train_line_number_total_encoded))\n",
        "\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(train_labels_encoded)\n",
        "\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val_sentences , \n",
        "                                               val_chars , \n",
        "                                               val_line_number_total_encoded))\n",
        "\n",
        "val_labels = tf.data.Dataset.from_tensor_slices(val_labels_encoded)\n",
        "\n",
        "# Zipping the data and labels \n",
        "train_dataset = tf.data.Dataset.zip((train_data , train_labels))\n",
        "val_dataset = tf.data.Dataset.zip((val_data , val_labels))\n",
        "\n",
        "# Applying batch and prefetching \n",
        "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufZc3ZfHkNby",
        "outputId": "f48a337a-58dc-4bb5-8156-1b864cb0a3ab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: (((None,), (None,), (None, 460)), (None,)), types: ((tf.string, tf.string, tf.int32), tf.int64)>,\n",
              " <PrefetchDataset shapes: (((None,), (None,), (None, 460)), (None,)), types: ((tf.string, tf.string, tf.int32), tf.int64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "bert_inputs = preprocessing(token_inputs)\n",
        "bert_embedding = bert(bert_inputs) \n",
        "token_outputs = layers.Dense(64, activation=\"relu\")(bert_embedding['pooled_output'])\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "input_char = layers.Input(shape = [] , dtype =tf.string)\n",
        "bert_inputs_char = preprocessing(input_char)\n",
        "bert_embedding_char =bert(bert_inputs_char)\n",
        "output_char = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
        "char_model = tf.keras.Model(input_char , output_char)\n",
        "\n",
        "# 3. Line numbers model\n",
        "# inputs, shape = ?\n",
        "line_total_inputs = layers.Input(shape=(460,), dtype=tf.float32, name=\"line_total_input\")\n",
        "# dense layer with 32 units & relu activation\n",
        "x = layers.Dense(32, activation=\"relu\")(line_total_inputs)\n",
        "# combine inputs & dense layer into model\n",
        "line_total_model = tf.keras.Model(inputs=line_total_inputs,\n",
        "                                   outputs=x)\n",
        "\n",
        "# 4. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"char_token_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "\n",
        "\n",
        "# Combining the line_number_total to our hybrid model (Time for Tribid!!)\n",
        "z = layers.Concatenate(name = 'tribid_embeddings')([line_total_model.output , \n",
        "                                                    combined_embeddings])\n",
        "\n",
        "# Adding a dense + dropout and creating our output layer \n",
        "dropout = layers.Dropout(0.5)(z)\n",
        "x = layers.Dense(128 , activation='relu')(dropout)\n",
        "output_layer = layers.Dense(5 , activation='softmax')(x)\n",
        "\n",
        "\n",
        "#Put together model with all kinds of inputs\n",
        "# Packing into a model\n",
        "model_line_total = tf.keras.Model(inputs = [token_model.input , \n",
        "                                        char_model.input , \n",
        "                                        line_total_model.input] , \n",
        "                              outputs = output_layer)\n"
      ],
      "metadata": {
        "id": "JcjQTmLNc3JH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile  model\n",
        "model_line_total.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                     optimizer= tf.keras.optimizers.Adam() , \n",
        "                     metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "PJQIDDkBdJYv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_line_total.fit(train_dataset , \n",
        "                 steps_per_epoch = int(0.1 * len(train_dataset)), \n",
        "                 epochs = 3 , \n",
        "                 validation_steps = int(0.1 * len(valid_dataset)),\n",
        "                 validation_data = valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PiNe40ig4-C",
        "outputId": "28fa05ab-19b2-444e-835c-a55783e52898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "  7/281 [..............................] - ETA: 2:54:28 - loss: 1.7242 - accuracy: 0.2939"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:"
      ],
      "metadata": {
        "id": "NuU9SiFx3lQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_sequence(abstract , model , label_encoder):\n",
        "  # Create sentencizer - Source: https://spacy.io/usage/linguistic-features#sbd \n",
        "  from spacy.lang.en import English\n",
        "  nlp = English() # setup English sentence parser\n",
        "  sentencizer = nlp.create_pipe(\"sentencizer\") # create sentence splitting pipeline object\n",
        "  nlp.add_pipe('sentencizer') # add sentence splitting pipeline object to sentence parser\n",
        "  doc = nlp(abstract[0][\"abstract\"]) # create \"doc\" of parsed sequences, change index for a different abstract\n",
        "  abstract_lines = [str(sent) for sent in list(doc.sents)] # return detected sentences from doc in string type (not spaCy token type)\n",
        "  \n",
        "  total_lines_in_sample = len(abstract_lines)\n",
        "  sample_lines = []\n",
        "  for i, line in enumerate(abstract_lines):\n",
        "    sample_dict = {}\n",
        "    sample_dict[\"text\"] = str(line)\n",
        "    sample_dict[\"line_number\"] = i\n",
        "    sample_dict[\"total_lines\"] = total_lines_in_sample - 1\n",
        "    sample_lines.append(sample_dict)\n",
        "\n",
        "  # Get all line_number values from sample abstract\n",
        "  test_abstract_line_numbers = [line[\"line_number\"] for line in sample_lines]\n",
        "  # One-hot encode to same depth as training data, so model accepts right input shape\n",
        "  test_abstract_line_numbers_one_hot = tf.one_hot(test_abstract_line_numbers, depth=15) \n",
        "  \n",
        "  test_abstract_total_lines = [line[\"total_lines\"] for line in sample_lines]\n",
        "  # One-hot encode to same depth as training data, so model accepts right input shape\n",
        "  test_abstract_total_lines_one_hot = tf.one_hot(test_abstract_total_lines, depth=20)\n",
        "\n",
        "  # Split abstract lines into characters\n",
        "  abstract_chars = [split_chars(sentence) for sentence in abstract_lines]\n",
        "\n",
        "  test_abstract_pred_probs = loaded_model.predict(x=(test_abstract_line_numbers_one_hot,\n",
        "                                                   test_abstract_total_lines_one_hot,\n",
        "                                                   tf.constant(abstract_lines),\n",
        "                                                   tf.constant(abstract_chars)))\n",
        "  \n",
        "  test_abstract_preds = tf.argmax(test_abstract_pred_probs, axis=1)\n",
        "  test_abstract_pred_classes = [label_encoder.classes_[i] for i in test_abstract_preds]\n",
        "\n",
        "  for i, line in enumerate(abstract_lines):\n",
        "    print(f\"{test_abstract_pred_classes[i]}: {line}\")\n"
      ],
      "metadata": {
        "id": "S5LQmYn48KhX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pEXvflMO7Cy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}